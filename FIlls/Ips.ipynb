{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1578aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from LHC_FillingPattern import LHCFillingPattern \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import math\n",
    "import pyarrow.dataset as ds\n",
    "from scipy.stats import linregress\n",
    "from pathlib import Path\n",
    "from lmfit import Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31066fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user settings\n",
    "beam = \"B1\"\n",
    "ip   = \"1\"\n",
    "RAWDATA = Path('/eos/project/l/lhc-lumimod/LuminosityFollowUp/2025/rawdata')\n",
    "FILLINFO       = Path(\"/eos/project/l/lhc-lumimod/LuminosityFollowUp/2025/fills-info\")\n",
    "fbmodes        = pd.read_parquet(FILLINFO / \"fills_and_bmodes_2025.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef04a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load variabled\n",
    "int_var   = f\"LHC.BCTFR.B6R4.{beam}:BUNCH_INTENSITY\"\n",
    "bsrt_loc = '5R4' if beam.upper() == 'B1' else '5L4'\n",
    "int_var   = f\"LHC.BCTFR.B6R4.{beam}:BUNCH_INTENSITY\"\n",
    "emitH_var = f\"LHC.BSRT.{bsrt_loc}.{beam}:BUNCH_EMITTANCE_H\"\n",
    "emitV_var = f\"LHC.BSRT.{bsrt_loc}.{beam}:BUNCH_EMITTANCE_V\"\n",
    "loss_b1 = 'UCAP.LHC.LUMI.LOSSES:EffectiveCrossSectionPerBunch:effectiveCrossSectionB1'\n",
    "loss_b2 = \"UCAP.LHC.LUMI.LOSSES:EffectiveCrossSectionPerBunch:effectiveCrossSectionB2\"\n",
    "lev_pv = \"LHC.LUMISERVER:LumiLevelingIP1:Enable\"\n",
    "beta_pv     = \"HX:BETASTAR_IP1\"\n",
    "xing_pv     = \"LhcStateTracker:LHCBEAM:IP1-XING-H-MURAD:value\"\n",
    "fills = np.array([10665,10666,10671,10673,10675,10676,10685,10689,10690,10701,10717,10721,10732,10709]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10af413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Build Families for ONE fill\n",
    "# ------------------------------------------------------------------\n",
    "def build_families_for_fill(\n",
    "    fillno: int,\n",
    "    beam: str,\n",
    "    ip: str,\n",
    "    RAWDATA: Path,\n",
    "    long_gap_set=(32, 63),\n",
    "    small_gap=8,\n",
    "    F4_offset_after_long=6,    # 7th bunch (bid_first + 6)\n",
    "    # F5_offset_after_long removed from use; Family_5 now from small-gap trains\n",
    "    F6_offset_before_small=26, # 27th bunch (bid_first + 26)\n",
    "    enforce_filled=True,\n",
    "    intensity_threshold=None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Return dict {Family_1: np.ndarray([...]), ..., Family_8: np.ndarray([...])}\n",
    "    for the requested fill/beam/ip.\n",
    "\n",
    "    Family_5 MODIFIED: 15th bunch (bid_first + 14) in trains whose *preceding* gap == small_gap (default 8).\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Fetch filling pattern object ---\n",
    "    fpat = LHCFillingPattern(fillno, RAWDATA)\n",
    "\n",
    "    # beam-specific train table & \"filled\" mask\n",
    "    if beam.upper() == \"B1\":\n",
    "        trains = fpat.bunchtrainsDF_b1.copy()\n",
    "        filled_slots = fpat.bunches_b1\n",
    "    else:\n",
    "        trains = fpat.bunchtrainsDF_b2.copy()\n",
    "        filled_slots = fpat.bunches_b2\n",
    "\n",
    "    trains = (\n",
    "        trains.loc[trains[\"id\"] != 0]\n",
    "              .sort_values(\"bid_first\")\n",
    "              .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    orbit_len = 3564\n",
    "\n",
    "    # --- optional BUNCH_INTENSITY for filtering ---\n",
    "    if intensity_threshold is not None:\n",
    "        int_var = f\"LHC.BCTFR.B6R4.{beam}:BUNCH_INTENSITY\"\n",
    "        ser_int = load_series(int_var, RAWDATA, fillno=fillno)  # helper below\n",
    "        if ser_int.empty:\n",
    "            arr_intensity = np.zeros(orbit_len, dtype=float)\n",
    "        else:\n",
    "            last_vals = np.asarray(ser_int.iloc[-1])\n",
    "            if last_vals.shape[0] != orbit_len:\n",
    "                tmp = np.zeros(orbit_len, dtype=float)\n",
    "                n = min(orbit_len, last_vals.shape[0])\n",
    "                tmp[:n] = last_vals[:n]\n",
    "                last_vals = tmp\n",
    "            arr_intensity = last_vals\n",
    "    else:\n",
    "        arr_intensity = None\n",
    "\n",
    "    # ---- filter helpers ------------------------------------------------\n",
    "    filled_mask = np.zeros(orbit_len, dtype=bool)\n",
    "    filled_mask[filled_slots] = True\n",
    "\n",
    "    def _filter(arr: np.ndarray) -> np.ndarray:\n",
    "        if arr.size == 0:\n",
    "            return arr\n",
    "        arr = arr.astype(int)\n",
    "        if enforce_filled:\n",
    "            arr = arr[filled_mask[arr]]\n",
    "        if arr_intensity is not None and intensity_threshold is not None:\n",
    "            arr = arr[arr_intensity[arr] >= intensity_threshold]\n",
    "        return arr\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Family_1: first bunch of trains whose *preceding* gap is long (32 or 63)\n",
    "    # ------------------------------------------------------------------\n",
    "    mask_long = trains[\"gap\"].isin(long_gap_set)\n",
    "    Family_1 = trains.loc[mask_long, \"bid_first\"].to_numpy(int)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Family_2: last bunch of a train that is followed by a long (≥31) gap\n",
    "    # ------------------------------------------------------------------\n",
    "    next_start = trains[\"bid_first\"].shift(-1).fillna(orbit_len).astype(int)\n",
    "    trains[\"gap_after\"] = next_start - trains[\"bid_last\"] - 1\n",
    "    mask_big_gap = trains[\"gap_after\"] >= 31\n",
    "    Family_2 = trains.loc[mask_big_gap, \"bid_last\"].to_numpy(int)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Family_3: first slot of max LR encounter per train\n",
    "    # ------------------------------------------------------------------\n",
    "    lr = fpat.lrencounters[beam][f\"ip{ip}\"]  # length-3564 array\n",
    "    peak_slots = []\n",
    "    for _, row in trains.iterrows():\n",
    "        bids = np.asarray(row[\"bids\"], dtype=int)\n",
    "        lr_train = lr[bids]\n",
    "        max_val = lr_train.max()\n",
    "        first_idx = np.where(lr_train == max_val)[0][0]\n",
    "        peak_slots.append(bids[first_idx])\n",
    "    Family_3 = np.asarray(peak_slots, dtype=int)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Family_4: 7th bunch in long-gap trains (bid_first + 6)\n",
    "    # ------------------------------------------------------------------\n",
    "    Family_4 = (trains.loc[mask_long, \"bid_first\"] + F4_offset_after_long).to_numpy(int)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Family_5 (MODIFIED): 15th bunch in trains whose *preceding* gap == small_gap\n",
    "    #                      (e.g., gap == 8). Require nbunches >= 15.\n",
    "    # ------------------------------------------------------------------\n",
    "    mask_gap_small_15 = (trains[\"gap\"] == small_gap) & (trains[\"nbunches\"] >= 15)\n",
    "    cand_F5 = (trains.loc[mask_gap_small_15, \"bid_first\"] + 14).to_numpy(int)\n",
    "    # clip to each selected train's end (safety)\n",
    "    last_F5 = trains.loc[mask_gap_small_15, \"bid_last\"].to_numpy(int)\n",
    "    Family_5 = cand_F5[cand_F5 <= last_F5]\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Family_6: 27th bunch IN THE TRAIN *BEFORE* a gap==8 train\n",
    "    # ------------------------------------------------------------------\n",
    "    rows_gap8 = trains[\"gap\"] == small_gap\n",
    "    train_before_gap8 = trains.shift(1).loc[rows_gap8].dropna()\n",
    "    mask_nbunch27 = train_before_gap8[\"nbunches\"] >= (F6_offset_before_small + 1)\n",
    "    Family_6 = (train_before_gap8.loc[mask_nbunch27, \"bid_first\"] + F6_offset_before_small).to_numpy(int)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Family_7: first bunch AFTER an 8-slot gap (the gap8 train itself)\n",
    "    # ------------------------------------------------------------------\n",
    "    Family_7 = trains.loc[rows_gap8, \"bid_first\"].to_numpy(int)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Family_8: last bunch BEFORE each gap8 (the last bunch of the previous train)\n",
    "    # ------------------------------------------------------------------\n",
    "    train_before_gap8_full = trains.shift(1).loc[rows_gap8].dropna()\n",
    "    Family_8 = train_before_gap8_full[\"bid_last\"].astype(int).to_numpy()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # apply filters\n",
    "    # ------------------------------------------------------------------\n",
    "    fam_dict = {\n",
    "        \"Family_1\": _filter(Family_1),\n",
    "        \"Family_2\": _filter(Family_2),\n",
    "        \"Family_3\": _filter(Family_3),\n",
    "        \"Family_4\": _filter(Family_4),\n",
    "        \"Family_5\": _filter(Family_5),\n",
    "        \"Family_6\": _filter(Family_6),\n",
    "        \"Family_7\": _filter(Family_7),\n",
    "        \"Family_8\": _filter(Family_8),\n",
    "    }\n",
    "\n",
    "    # report\n",
    "    #print(f\"\\n=== Families for fill {fillno} ({beam}, {ip}) ===\")\n",
    "    ##for k, v in fam_dict.items():\n",
    "        #print(f\"{k}: n={len(v)}  slots={v}\")\n",
    "\n",
    "    return fam_dict\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Minimal load_series helper (same as in larger module)\n",
    "# ------------------------------------------------------------------\n",
    "def load_series(var: str, RAWDATA: Path, fillno: int) -> pd.Series:\n",
    "    root = RAWDATA / f\"HX:FILLN={fillno}\"\n",
    "    parts = []\n",
    "    for pq in root.rglob(\"*.parquet\"):\n",
    "        try:\n",
    "            df = pd.read_parquet(pq, columns=[var])\n",
    "        except Exception:\n",
    "            continue\n",
    "        idx = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
    "        ser = pd.Series(df[var].to_numpy(), index=idx, name=var)\n",
    "        parts.append(ser.dropna())\n",
    "    if not parts:\n",
    "        return pd.Series(dtype=float, name=var)\n",
    "    return pd.concat(parts).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74623eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_families_wide shape: (14, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fill</th>\n",
       "      <th>Family_1</th>\n",
       "      <th>Family_2</th>\n",
       "      <th>Family_3</th>\n",
       "      <th>Family_4</th>\n",
       "      <th>Family_5</th>\n",
       "      <th>Family_6</th>\n",
       "      <th>Family_7</th>\n",
       "      <th>Family_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10665</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10666</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10671</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10673</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10675</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10676</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10685</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10689</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10690</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10701</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10717</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10721</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10732</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10709</td>\n",
       "      <td>[55, 208, 404, 600, 796, 949, 1102, 1298, 1494...</td>\n",
       "      <td>[176, 372, 568, 764, 917, 1070, 1266, 1462, 16...</td>\n",
       "      <td>[71, 114, 157, 224, 267, 310, 353, 420, 463, 5...</td>\n",
       "      <td>[61, 214, 410, 606, 802, 955, 1108, 1304, 1500...</td>\n",
       "      <td>[112, 155, 265, 308, 351, 461, 504, 547, 657, ...</td>\n",
       "      <td>[81, 124, 234, 277, 320, 430, 473, 516, 626, 6...</td>\n",
       "      <td>[98, 141, 251, 294, 337, 447, 490, 533, 643, 6...</td>\n",
       "      <td>[90, 133, 243, 286, 329, 439, 482, 525, 635, 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fill                                           Family_1  \\\n",
       "0   10665  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "1   10666  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "2   10671  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "3   10673  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "4   10675  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "5   10676  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "6   10685  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "7   10689  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "8   10690  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "9   10701  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "10  10717  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "11  10721  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "12  10732  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "13  10709  [55, 208, 404, 600, 796, 949, 1102, 1298, 1494...   \n",
       "\n",
       "                                             Family_2  \\\n",
       "0   [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "1   [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "2   [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "3   [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "4   [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "5   [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "6   [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "7   [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "8   [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "9   [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "10  [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "11  [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "12  [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "13  [176, 372, 568, 764, 917, 1070, 1266, 1462, 16...   \n",
       "\n",
       "                                             Family_3  \\\n",
       "0   [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "1   [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "2   [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "3   [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "4   [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "5   [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "6   [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "7   [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "8   [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "9   [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "10  [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "11  [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "12  [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "13  [71, 114, 157, 224, 267, 310, 353, 420, 463, 5...   \n",
       "\n",
       "                                             Family_4  \\\n",
       "0   [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "1   [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "2   [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "3   [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "4   [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "5   [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "6   [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "7   [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "8   [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "9   [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "10  [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "11  [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "12  [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "13  [61, 214, 410, 606, 802, 955, 1108, 1304, 1500...   \n",
       "\n",
       "                                             Family_5  \\\n",
       "0   [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "1   [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "2   [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "3   [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "4   [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "5   [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "6   [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "7   [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "8   [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "9   [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "10  [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "11  [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "12  [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "13  [112, 155, 265, 308, 351, 461, 504, 547, 657, ...   \n",
       "\n",
       "                                             Family_6  \\\n",
       "0   [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "1   [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "2   [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "3   [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "4   [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "5   [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "6   [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "7   [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "8   [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "9   [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "10  [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "11  [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "12  [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "13  [81, 124, 234, 277, 320, 430, 473, 516, 626, 6...   \n",
       "\n",
       "                                             Family_7  \\\n",
       "0   [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "1   [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "2   [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "3   [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "4   [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "5   [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "6   [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "7   [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "8   [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "9   [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "10  [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "11  [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "12  [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "13  [98, 141, 251, 294, 337, 447, 490, 533, 643, 6...   \n",
       "\n",
       "                                             Family_8  \n",
       "0   [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "1   [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "2   [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "3   [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "4   [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "5   [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "6   [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "7   [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "8   [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "9   [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "10  [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "11  [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "12  [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  \n",
       "13  [90, 133, 243, 286, 329, 439, 482, 525, 635, 6...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fams_by_fill = {}\n",
    "records = []   # for long DataFrame\n",
    "\n",
    "for fillno in fills:\n",
    "    fams = build_families_for_fill(\n",
    "        fillno=fillno,\n",
    "        beam=beam,\n",
    "        ip=ip,\n",
    "        RAWDATA=RAWDATA,\n",
    "        # (optionally pass thresholds/offset overrides if needed)\n",
    "    )\n",
    "    fams_by_fill[fillno] = fams\n",
    "# ------------------------------------------------------------------\n",
    "# group back by fill -> dict, then assemble as columns\n",
    "wide_rows = []\n",
    "for fillno, fams in fams_by_fill.items():\n",
    "    row = {\"fill\": fillno}\n",
    "    for fam_name, slots in fams.items():\n",
    "        row[fam_name] = np.array(slots, dtype=int)  # ragged arrays stored as objects\n",
    "    wide_rows.append(row)\n",
    "\n",
    "df_families_wide = pd.DataFrame(wide_rows)\n",
    "print(\"df_families_wide shape:\", df_families_wide.shape)\n",
    "display(df_families_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c33bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_families_ip_long shape: (4494, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fill</th>\n",
       "      <th>family</th>\n",
       "      <th>ip_group</th>\n",
       "      <th>slot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_1</td>\n",
       "      <td>15_2_8</td>\n",
       "      <td>1843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_1</td>\n",
       "      <td>15_2_8</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_1</td>\n",
       "      <td>15_2_8</td>\n",
       "      <td>2192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_1</td>\n",
       "      <td>15_2_8</td>\n",
       "      <td>2388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_1</td>\n",
       "      <td>15_8</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fill    family ip_group  slot\n",
       "0  10665  Family_1   15_2_8  1843\n",
       "1  10665  Family_1   15_2_8  1996\n",
       "2  10665  Family_1   15_2_8  2192\n",
       "3  10665  Family_1   15_2_8  2388\n",
       "4  10665  Family_1     15_8    55"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_families_ip_summary shape: (112, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fill</th>\n",
       "      <th>family</th>\n",
       "      <th>15_only</th>\n",
       "      <th>2_only</th>\n",
       "      <th>8_only</th>\n",
       "      <th>15_2</th>\n",
       "      <th>15_8</th>\n",
       "      <th>2_8</th>\n",
       "      <th>15_2_8</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10665</td>\n",
       "      <td>Family_8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10666</td>\n",
       "      <td>Family_1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fill    family  15_only  2_only  8_only  15_2  15_8  2_8  15_2_8  none\n",
       "0  10665  Family_1        5       0       0     0    10    0       4     0\n",
       "1  10665  Family_2        0       0       0     1     6    0      12     0\n",
       "2  10665  Family_3        0       0       0     2     3    0      63     0\n",
       "3  10665  Family_4        0       0       0     5     1    0      13     0\n",
       "4  10665  Family_5        0       0       0     1     2    0      46     0\n",
       "5  10665  Family_6        0       0       0     2     1    0      46     0\n",
       "6  10665  Family_7        2       0       0     0    36    0      11     0\n",
       "7  10665  Family_8        0       0       0     2     1    0      46     0\n",
       "8  10666  Family_1        5       0       0     0    10    0       4     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _ip_collision_sets_for_fill(fillno: int, beam: str, RAWDATA: Path):\n",
    "    \"\"\"\n",
    "    Return dict with Python sets for slots colliding at each IP-class:\n",
    "        {'15': set(...), '2': set(...), '8': set(...)}\n",
    "    where '15' is the combined IP1+IP5 mask exposed by fpat.collbid_ip15.\n",
    "    \"\"\"\n",
    "    fpat = LHCFillingPattern(fillno, RAWDATA)\n",
    "\n",
    "    if beam.upper() == \"B1\":\n",
    "        ip15 = np.asarray(fpat.collbid_ip15[\"B1\"], dtype=int)\n",
    "        ip2  = np.asarray(fpat.collbid_ip2 [\"B1\"], dtype=int)\n",
    "        ip8  = np.asarray(fpat.collbid_ip8 [\"B1\"], dtype=int)\n",
    "    else:\n",
    "        ip15 = np.asarray(fpat.collbid_ip15[\"B2\"], dtype=int)\n",
    "        ip2  = np.asarray(fpat.collbid_ip2 [\"B2\"], dtype=int)\n",
    "        ip8  = np.asarray(fpat.collbid_ip8 [\"B2\"], dtype=int)\n",
    "\n",
    "    return {\n",
    "        \"15\": set(ip15),\n",
    "        \"2\":  set(ip2),\n",
    "        \"8\":  set(ip8),\n",
    "    }\n",
    "\n",
    "# Classify a numpy array of slots into exclusive IP groups\n",
    "\n",
    "def _classify_slots_by_ip(slots: np.ndarray, set15, set2, set8):\n",
    "    \"\"\"\n",
    "    Given 1D int array `slots` and three sets (IP15, IP2, IP8),\n",
    "    return dict {ip_group_label: np.ndarray([...])}.\n",
    "    \"\"\"\n",
    "    slots = np.asarray(slots, dtype=int)\n",
    "\n",
    "    # for each slot, boolean membership\n",
    "    in15 = np.array([s in set15 for s in slots], dtype=bool)\n",
    "    in2  = np.array([s in set2  for s in slots], dtype=bool)\n",
    "    in8  = np.array([s in set8  for s in slots], dtype=bool)\n",
    "\n",
    "    # masks\n",
    "    m15_only   =  in15 & ~in2 & ~in8\n",
    "    m2_only    = ~in15 &  in2 & ~in8\n",
    "    m8_only    = ~in15 & ~in2 &  in8\n",
    "\n",
    "    m15_2      =  in15 &  in2 & ~in8\n",
    "    m15_8      =  in15 & ~in2 &  in8\n",
    "    m2_8       = ~in15 &  in2 &  in8\n",
    "\n",
    "    m15_2_8    =  in15 &  in2 &  in8\n",
    "\n",
    "    # any that did not match any of above (should be none or \"non colliding\")\n",
    "    mnone      = ~(m15_only | m2_only | m8_only | m15_2 | m15_8 | m2_8 | m15_2_8)\n",
    "\n",
    "    return {\n",
    "        \"15_only\":   slots[m15_only],\n",
    "        \"2_only\":    slots[m2_only],\n",
    "        \"8_only\":    slots[m8_only],\n",
    "        \"15_2\":      slots[m15_2],\n",
    "        \"15_8\":      slots[m15_8],\n",
    "        \"2_8\":       slots[m2_8],\n",
    "        \"15_2_8\":    slots[m15_2_8],\n",
    "        \"none\":      slots[mnone],\n",
    "    }\n",
    "\n",
    "# Main: explode fams_by_fill into IP-partitioned long & summary DFs\n",
    "def build_family_ip_partitions(\n",
    "    fams_by_fill: dict,\n",
    "    RAWDATA: Path,\n",
    "    beam: str,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    fams_by_fill : dict\n",
    "        {fill: {\"Family_1\": array([...]), ...}}\n",
    "    RAWDATA : Path\n",
    "        root rawdata path\n",
    "    beam : str\n",
    "        'B1' or 'B2'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_families_ip_long : DataFrame\n",
    "        columns: fill, family, ip_group, slot\n",
    "    df_families_ip_summary : DataFrame\n",
    "        columns: fill, family, 15_only, 2_only, 8_only, 15_2, 15_8, 2_8, 15_2_8, none\n",
    "        counts per group.\n",
    "    \"\"\"\n",
    "    long_records = []\n",
    "    summary_records = []\n",
    "\n",
    "    for fillno, fams in fams_by_fill.items():\n",
    "        # Get IP sets for this fill\n",
    "        ip_sets = _ip_collision_sets_for_fill(fillno, beam, RAWDATA)\n",
    "        set15, set2, set8 = ip_sets[\"15\"], ip_sets[\"2\"], ip_sets[\"8\"]\n",
    "\n",
    "        for fam_name, slots in fams.items():\n",
    "            slots = np.asarray(slots, dtype=int)\n",
    "\n",
    "            part_dict = _classify_slots_by_ip(slots, set15, set2, set8)\n",
    "\n",
    "            # long rows\n",
    "            for ip_group, arr in part_dict.items():\n",
    "                for s in arr:\n",
    "                    long_records.append({\n",
    "                        \"fill\": fillno,\n",
    "                        \"family\": fam_name,\n",
    "                        \"ip_group\": ip_group,\n",
    "                        \"slot\": int(s),\n",
    "                    })\n",
    "\n",
    "            # summary counts\n",
    "            rec = {\n",
    "                \"fill\": fillno,\n",
    "                \"family\": fam_name,\n",
    "            }\n",
    "            for ip_group, arr in part_dict.items():\n",
    "                rec[ip_group] = int(len(arr))\n",
    "            summary_records.append(rec)\n",
    "\n",
    "    # assemble long\n",
    "    if long_records:\n",
    "        df_families_ip_long = (\n",
    "            pd.DataFrame.from_records(long_records)\n",
    "              .sort_values([\"fill\", \"family\", \"ip_group\", \"slot\"])\n",
    "              .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        df_families_ip_long = pd.DataFrame(columns=[\"fill\", \"family\", \"ip_group\", \"slot\"])\n",
    "\n",
    "    # assemble summary\n",
    "    if summary_records:\n",
    "        df_families_ip_summary = (\n",
    "            pd.DataFrame.from_records(summary_records)\n",
    "              .sort_values([\"fill\", \"family\"])\n",
    "              .reset_index(drop=True)\n",
    "        )\n",
    "    else:\n",
    "        df_families_ip_summary = pd.DataFrame(columns=[\"fill\", \"family\"])\n",
    "\n",
    "    return df_families_ip_long, df_families_ip_summary\n",
    "\n",
    "df_families_ip_long, df_families_ip_summary = build_family_ip_partitions(\n",
    "    fams_by_fill=fams_by_fill,\n",
    "    RAWDATA=RAWDATA,\n",
    "    beam=beam,          # same beam as used when building families\n",
    ")\n",
    "\n",
    "print(\"df_families_ip_long shape:\", df_families_ip_long.shape)\n",
    "display(df_families_ip_long.head())\n",
    "\n",
    "print(\"df_families_ip_summary shape:\", df_families_ip_summary.shape)\n",
    "display(df_families_ip_summary.head(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7a6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "ip_partition_family_slopes.py\n",
    "=============================\n",
    "\n",
    "Robust bunch-emittance growth-rate analysis:\n",
    "\n",
    "1.  Build families-by-IP partitions for every fill.\n",
    "2.  Fit weighted, robust slopes per slot.\n",
    "3.  Aggregate to family-level, then average over fills.\n",
    "4.  Make one figure per IP-group with families on the x-axis.\n",
    "\n",
    "Replace the placeholders in `__main__` with your real data handles\n",
    "(fbmodes DataFrame, RAWDATA path, list/iterable of fills, etc.)\n",
    "and just run `python ip_partition_family_slopes.py`.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Imports & compatibility\n",
    "# --------------------------------------------------------------------------\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Mapping, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lmfit import Model\n",
    "\n",
    "# -------- Python < 3 .11: supply a minimal StrEnum ------------------------\n",
    "try:\n",
    "    from enum import StrEnum          # Py 3.11+\n",
    "except ImportError:                    # Py ≤ 3.10\n",
    "    from enum import Enum\n",
    "\n",
    "    class StrEnum(str, Enum):  # type: ignore\n",
    "        \"\"\"Fallback for Python versions lacking enum.StrEnum.\"\"\"\n",
    "        def _generate_next_value_(name, start, count, last_values):\n",
    "            return name\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# CONFIG — tweak to taste\n",
    "# --------------------------------------------------------------------------\n",
    "THRESHOLD        = 1e11       # intensity cut to define t0\n",
    "PLANE            = \"H\"        # default plane for summary plots\n",
    "FIGSIZE          = (9, 3)\n",
    "\n",
    "F_SCALE          = 0.1\n",
    "REL_ERR          = 0.05       # 5 % rel error\n",
    "ABS_ERR_FLOOR    = 0.02       # emittance units\n",
    "MIN_ERR          = 1e-12\n",
    "MIN_PTS          = 5\n",
    "MIN_DT_H         = 0.15       # hours  (≈ 9 min)\n",
    "MAX_WEIGHT       = 1e4\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 0.  LHCFillingPattern → IP-collision sets\n",
    "# --------------------------------------------------------------------------\n",
    "def _ip_collision_sets_for_fill(fillno: int, beam: str, rawdata: Path\n",
    ") -> dict[str, set[int]]:\n",
    "    \"\"\"\n",
    "    Return a dict of *sets* of bunch slots involved in collisions at each IP-class.\n",
    "\n",
    "    Keys:\n",
    "        \"15\" – combined IP1 + IP5 mask (`fpat.collbid_ip15`)\n",
    "        \"2\"  – IP2 mask\n",
    "        \"8\"  – IP8 mask\n",
    "    \"\"\"\n",
    "\n",
    "    fpat = LHCFillingPattern(fillno, rawdata)\n",
    "    side = \"B1\" if beam.upper() == \"B1\" else \"B2\"\n",
    "\n",
    "    ip15 = fpat.collbid_ip15[side]\n",
    "    ip2  = fpat.collbid_ip2 [side]\n",
    "    ip8  = fpat.collbid_ip8 [side]\n",
    "\n",
    "    return {\"15\": set(map(int, ip15)),\n",
    "            \"2\":  set(map(int, ip2)),\n",
    "            \"8\":  set(map(int, ip8))}\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 1.  Classify arbitrary slot arrays into mutually-exclusive IP groups\n",
    "# --------------------------------------------------------------------------\n",
    "def _classify_slots_by_ip(\n",
    "    slots: Sequence[int],\n",
    "    set15: set[int], set2: set[int], set8: set[int],\n",
    ") -> dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    slots : 1-D iterable of integers\n",
    "    set15,set2,set8 : sets of ints returned by `_ip_collision_sets_for_fill`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[label → ndarray[int]]\n",
    "        Arrays are *sub-views* of `slots` in the original order.\n",
    "    \"\"\"\n",
    "    slots = np.asarray(slots, dtype=int)\n",
    "\n",
    "    # fast vectorised membership\n",
    "    in15, in2, in8 = (np.isin(slots, list(s)) for s in (set15, set2, set8))\n",
    "\n",
    "    m15_only   =  in15 & ~in2 & ~in8\n",
    "    m2_only    = ~in15 &  in2 & ~in8\n",
    "    m8_only    = ~in15 & ~in2 &  in8\n",
    "    m15_2      =  in15 &  in2 & ~in8\n",
    "    m15_8      =  in15 & ~in2 &  in8\n",
    "    m2_8       = ~in15 &  in2 &  in8\n",
    "    m15_2_8    =  in15 &  in2 &  in8\n",
    "    mnone      = ~(in15 | in2 | in8)\n",
    "\n",
    "    return {\n",
    "        \"15_only\": slots[m15_only],\n",
    "        \"2_only\":  slots[m2_only],\n",
    "        \"8_only\":  slots[m8_only],\n",
    "        \"15_2\":    slots[m15_2],\n",
    "        \"15_8\":    slots[m15_8],\n",
    "        \"2_8\":     slots[m2_8],\n",
    "        \"15_2_8\":  slots[m15_2_8],\n",
    "        \"none\":    slots[mnone],\n",
    "    }\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.  Main: explode `fams_by_fill` into IP-partitioned DataFrames\n",
    "# --------------------------------------------------------------------------\n",
    "def build_family_ip_partitions(\n",
    "    fams_by_fill: Mapping[int, Mapping[str, Sequence[int]]],\n",
    "    rawdata: Path,\n",
    "    beam: str,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    df_ip_long : columns (fill, family, ip_group, slot)\n",
    "    df_ip_summary : columns (fill, family, …group counts…)\n",
    "    \"\"\"\n",
    "    long_records, summary_records = [], []\n",
    "\n",
    "    for fillno, fams in fams_by_fill.items():\n",
    "        ip_sets = _ip_collision_sets_for_fill(fillno, beam, rawdata)\n",
    "        set15, set2, set8 = ip_sets[\"15\"], ip_sets[\"2\"], ip_sets[\"8\"]\n",
    "\n",
    "        for fam, slots in fams.items():\n",
    "            slots = np.asarray(slots, dtype=int)\n",
    "            partitions = _classify_slots_by_ip(slots, set15, set2, set8)\n",
    "\n",
    "            # long form\n",
    "            for ipg, arr in partitions.items():\n",
    "                for s in arr:\n",
    "                    long_records.append({\"fill\": fillno,\n",
    "                                         \"family\": fam,\n",
    "                                         \"ip_group\": ipg,\n",
    "                                         \"slot\": int(s)})\n",
    "            # summary counts\n",
    "            rec = {\"fill\": fillno, \"family\": fam}\n",
    "            rec.update({ipg: len(arr) for ipg, arr in partitions.items()})\n",
    "            summary_records.append(rec)\n",
    "\n",
    "    df_long = (pd.DataFrame(long_records)\n",
    "                 .sort_values([\"fill\", \"family\", \"ip_group\", \"slot\"])\n",
    "                 .reset_index(drop=True))\n",
    "    df_sum  = (pd.DataFrame(summary_records)\n",
    "                 .sort_values([\"fill\", \"family\"])\n",
    "                 .reset_index(drop=True))\n",
    "    return df_long, df_sum\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.  Robust weighted line fit\n",
    "# --------------------------------------------------------------------------\n",
    "_line_model = Model(lambda x, m, b: m * x + b)\n",
    "\n",
    "def robust_line(x_hours: np.ndarray, y: np.ndarray, f_scale=F_SCALE):\n",
    "    yerr = np.maximum(np.maximum(REL_ERR * np.abs(y), ABS_ERR_FLOOR), MIN_ERR)\n",
    "    w = 1.0 / yerr\n",
    "    w = np.minimum(w, MAX_WEIGHT)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"lmfit\")\n",
    "        res = _line_model.fit(\n",
    "            y, x=x_hours,\n",
    "            m=0.0, b=y[0],\n",
    "            weights=w,\n",
    "            method=\"least_squares\",\n",
    "            fit_kws={\"loss\": \"soft_l1\", \"f_scale\": f_scale},\n",
    "        )\n",
    "\n",
    "    m = res.params[\"m\"].value\n",
    "    b = res.params[\"b\"].value\n",
    "    sm = res.params[\"m\"].stderr or np.nan\n",
    "    return m, b, sm, res\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4.  Per-fill family-level robust slopes\n",
    "# --------------------------------------------------------------------------\n",
    "def _wmean_and_ci(pairs: Sequence[tuple[float, float]]\n",
    ") -> tuple[float, float, float, float, float, float, int]:\n",
    "    \"\"\"Weighted mean ± CI, plus spread stats, from (value, sigma) pairs.\"\"\"\n",
    "    if not pairs:\n",
    "        return (np.nan,) * 6 + (0,)\n",
    "\n",
    "    vals, sigs = map(np.asarray, zip(*pairs))\n",
    "    ok = np.isfinite(vals) & np.isfinite(sigs) & (sigs > 0)\n",
    "    if ok.sum() == 0:\n",
    "        return (np.nan,) * 6 + (0,)\n",
    "\n",
    "    w = 1.0 / sigs[ok] ** 2\n",
    "    mean = np.sum(w * vals[ok]) / np.sum(w)\n",
    "    sem  = (1.0 / np.sum(w)) ** 0.5\n",
    "    ci95 = 1.96 * sem\n",
    "    std  = np.std(vals[ok], ddof=1) if ok.sum() > 1 else np.nan\n",
    "    mad  = 1.4826 * np.median(np.abs(vals[ok] - np.median(vals[ok]))) \\\n",
    "           if ok.sum() > 1 else np.nan\n",
    "    return mean, sem, mean - ci95, mean + ci95, std, mad, int(ok.sum())\n",
    "\n",
    "\n",
    "def one_fill_family_rates_robust(\n",
    "    fno: int,\n",
    "    beam: str,\n",
    "    families: Mapping[str, Sequence[int]],\n",
    "    fbmodes: pd.DataFrame,\n",
    "    rawdata: Path,\n",
    "    threshold: float = THRESHOLD,\n",
    ") -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    df : index (fill, family) with H_*/V_* statistics\n",
    "    fits : nested dict[fam][slot] → (H tuple, V tuple)  (for drill-downs)\n",
    "    \"\"\"\n",
    "    bsrt_loc = \"5R4\" if beam.upper() == \"B1\" else \"5L4\"\n",
    "\n",
    "    int_var = f\"LHC.BCTFR.B6R4.{beam}:BUNCH_INTENSITY\"\n",
    "    H_var = f\"LHC.BSRT.{bsrt_loc}.{beam}:BUNCH_EMITTANCE_H\"\n",
    "    V_var = f\"LHC.BSRT.{bsrt_loc}.{beam}:BUNCH_EMITTANCE_V\"\n",
    "\n",
    "    def load_INJPHYS(var: str) -> pd.Series:\n",
    "        sub = fbmodes.loc[fno]\n",
    "        rows = sub.query(\"BMODE=='INJPHYS'\").sort_values(\"tsStart\")\n",
    "        if rows.empty:\n",
    "            rows = sub.sort_values(\"tsStart\").iloc[[0]]\n",
    "        t0, t1 = map(pd.to_datetime, (rows[\"tsStart\"].iloc[0],\n",
    "                                      rows[\"tsEnd\"].iloc[0]))\n",
    "        parts = []\n",
    "        pq_dir = rawdata / f\"HX:FILLN={fno}\"\n",
    "        for pq in pq_dir.rglob(\"*.parquet\"):\n",
    "            try:\n",
    "                df = pd.read_parquet(pq, columns=[var])\n",
    "            except Exception as exc:\n",
    "                warnings.warn(f\"Skipping {pq} ({exc})\", UserWarning)\n",
    "                continue\n",
    "            ser = df[var]\n",
    "            ser.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "            parts.append(ser.dropna())\n",
    "        if not parts:\n",
    "            raise RuntimeError(f\"No data for PV {var!r} in fill {fno}\")\n",
    "        full = pd.concat(parts).sort_index()\n",
    "        return full.loc[t0:t1]\n",
    "\n",
    "    def extract_slot(ser: pd.Series, slot: int) -> pd.Series:\n",
    "        return ser.apply(\n",
    "            lambda a: a[slot] if hasattr(a, \"__len__\") and len(a) > slot else np.nan\n",
    "        )\n",
    "\n",
    "    def segment_ok(seg: pd.Series) -> bool:\n",
    "        if seg is None or seg.empty or len(seg) < MIN_PTS:\n",
    "            return False\n",
    "        dt_h = (seg.index[-1] - seg.index[0]).total_seconds() / 3600.0\n",
    "        return dt_h >= MIN_DT_H\n",
    "\n",
    "    I  = load_INJPHYS(int_var)\n",
    "    EH = load_INJPHYS(H_var)\n",
    "    EV = load_INJPHYS(V_var)\n",
    "\n",
    "    per_h: dict[int, tuple[float, float]] = {}\n",
    "    per_v: dict[int, tuple[float, float]] = {}\n",
    "    fits: dict[str, dict[int, dict[str, tuple]]] = {}\n",
    "\n",
    "    for fam, slots in families.items():\n",
    "        fits[fam] = {}\n",
    "        for slot in slots:\n",
    "            Ii = extract_slot(I, slot)\n",
    "            above = Ii[Ii >= threshold]\n",
    "            if above.empty:\n",
    "                per_h[slot] = per_v[slot] = (np.nan, np.nan)\n",
    "                continue\n",
    "            t0 = above.index[0]\n",
    "            Hs = extract_slot(EH, slot).loc[t0:].dropna()\n",
    "            Vs = extract_slot(EV, slot).loc[t0:].dropna()\n",
    "\n",
    "            if not (segment_ok(Hs) and segment_ok(Vs)):\n",
    "                per_h[slot] = per_v[slot] = (np.nan, np.nan)\n",
    "                continue\n",
    "\n",
    "            xh = (Hs.index - t0).total_seconds() / 3600.0\n",
    "            xv = (Vs.index - t0).total_seconds() / 3600.0\n",
    "            sh, ih, sh_err, _ = robust_line(xh, Hs.values)\n",
    "            sv, iv, sv_err, _ = robust_line(xv, Vs.values)\n",
    "            per_h[slot] = (sh, sh_err)\n",
    "            per_v[slot] = (sv, sv_err)\n",
    "            fits[fam][slot] = {\"H\": (sh, ih, sh_err, t0, Hs),\n",
    "                               \"V\": (sv, iv, sv_err, t0, Vs)}\n",
    "\n",
    "    rows = []\n",
    "    for fam, slots in families.items():\n",
    "        h_pairs = [per_h[s] for s in slots if s in per_h]\n",
    "        v_pairs = [per_v[s] for s in slots if s in per_v]\n",
    "\n",
    "        H_mean, H_sem, H_lo95, H_hi95, H_std, H_mad, H_n = _wmean_and_ci(h_pairs)\n",
    "        V_mean, V_sem, V_lo95, V_hi95, V_std, V_mad, V_n = _wmean_and_ci(v_pairs)\n",
    "\n",
    "        rows.append({\n",
    "            \"fill\": fno,\n",
    "            \"family\": fam,\n",
    "            \"H_rate_mean_w\": H_mean,\n",
    "            \"H_err_family\":  H_sem,\n",
    "            \"H_lo95\":        H_lo95,\n",
    "            \"H_hi95\":        H_hi95,\n",
    "            \"H_std_slots\":   H_std,\n",
    "            \"H_mad_slots\":   H_mad,\n",
    "            \"H_n_slots\":     H_n,\n",
    "            \"V_rate_mean_w\": V_mean,\n",
    "            \"V_err_family\":  V_sem,\n",
    "            \"V_lo95\":        V_lo95,\n",
    "            \"V_hi95\":        V_hi95,\n",
    "            \"V_std_slots\":   V_std,\n",
    "            \"V_mad_slots\":   V_mad,\n",
    "            \"V_n_slots\":     V_n,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index([\"fill\", \"family\"])\n",
    "    return df, fits\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 5.  Plot helpers\n",
    "# --------------------------------------------------------------------------\n",
    "def plot_slot_with_fit(slot_fit_tuple, plane=\"H\", figsize=FIGSIZE, label=None):\n",
    "    slope, intercept, _, t0, ser = slot_fit_tuple\n",
    "    x = (ser.index - t0).total_seconds() / 3600.0\n",
    "    y_fit = intercept + slope * x\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(ser.index, ser.values, label=\"Raw\", alpha=0.6)\n",
    "    plt.plot(ser.index, y_fit, label=\"Weighted robust fit\", linewidth=2)\n",
    "    plt.xlabel(\"Time\"); plt.ylabel(f\"Emittance {plane}\")\n",
    "    plt.title(label or \"Slot\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "def plot_ip_group(\n",
    "    df_mean: pd.DataFrame,\n",
    "    plane: str = \"H\",\n",
    "    ipg: str = \"15_only\",\n",
    "    err: str = \"sem\",\n",
    "    figsize: tuple[int, int] = (11, 3),\n",
    "):\n",
    "    \"\"\"One bar-plot panel for a given IP-group over all families.\"\"\"\n",
    "    sub = df_mean.query(\"ip_group == @ipg\")\n",
    "    fams = sub[\"family\"]\n",
    "    means = sub[f\"{plane}_mean\"]\n",
    "    if err.lower() == \"sem\":\n",
    "        yerr = sub[f\"{plane}_sem\"]\n",
    "        label = \"±1 SEM over fills\"\n",
    "    else:\n",
    "        ci = 1.96 * sub[f\"{plane}_sem\"]\n",
    "        yerr = np.vstack([ci, ci])\n",
    "        label = \"±95 % CI\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.errorbar(fams, means, yerr=yerr, fmt=\"o\", capsize=3,\n",
    "                elinewidth=1.5, label=label)\n",
    "    ax.set_xticklabels(fams, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(f\"{plane}-plane growth rate\")\n",
    "    ax.set_title(f\"IP-group {ipg} ({plane})\")\n",
    "    ax.legend(); fig.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 6.  High-level driver: analyse *and* plot\n",
    "# --------------------------------------------------------------------------\n",
    "def analyse_and_plot(\n",
    "    fills: Iterable[int],\n",
    "    beam: str,\n",
    "    ip: int | str,\n",
    "    rawdata: Path,\n",
    "    fbmodes: pd.DataFrame,\n",
    "    build_families_for_fill,   # user-provided\n",
    "    plane: str = PLANE,\n",
    "    threshold: float = THRESHOLD,\n",
    "):\n",
    "    \"\"\"\n",
    "    Full pipeline entry-point; returns the averaged DataFrame\n",
    "    and a nested dict of fits for drill-downs.\n",
    "    \"\"\"\n",
    "    # --------------------------------------------\n",
    "    # A. build plain families → fams_by_fill\n",
    "    # --------------------------------------------\n",
    "    fams_by_fill = {\n",
    "        fno: build_families_for_fill(fno, beam=beam, ip=ip, RAWDATA=rawdata)\n",
    "        for fno in fills\n",
    "    }\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # B. explode into IP-partitioned slot table\n",
    "    # --------------------------------------------\n",
    "    df_ip_long, _ = build_family_ip_partitions(\n",
    "        fams_by_fill=fams_by_fill, rawdata=rawdata, beam=beam\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # C. per-(fill × IP) robust rates\n",
    "    # --------------------------------------------\n",
    "    ip_tables, ip_fits = [], {}\n",
    "    for fno in fills:\n",
    "        df_fill = df_ip_long.query(\"fill == @fno\")\n",
    "        for ipg, grp in df_fill.groupby(\"ip_group\"):\n",
    "            fams_ip = {fam: g[\"slot\"].to_numpy()\n",
    "                       for fam, g in grp.groupby(\"family\")}\n",
    "            df_rates, fits_this = one_fill_family_rates_robust(\n",
    "                fno, beam, fams_ip, fbmodes, rawdata, threshold\n",
    "            )\n",
    "            df_rates = df_rates.reset_index()\n",
    "            df_rates[\"ip_group\"] = ipg\n",
    "            ip_tables.append(df_rates)\n",
    "            ip_fits.setdefault(ipg, {})[fno] = fits_this\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # D. average over fills\n",
    "    # --------------------------------------------\n",
    "    df_all = (pd.concat(ip_tables, ignore_index=True)\n",
    "                .set_index([\"ip_group\", \"family\", \"fill\"])\n",
    "                .sort_index())\n",
    "\n",
    "    df_mean = (df_all\n",
    "                 .groupby([\"ip_group\", \"family\"])\n",
    "                 .agg(H_mean=(\"H_rate_mean_w\", \"mean\"),\n",
    "                      H_sem =(\"H_rate_mean_w\", \"sem\"),\n",
    "                      V_mean=(\"V_rate_mean_w\", \"mean\"),\n",
    "                      V_sem =(\"V_rate_mean_w\", \"sem\"),\n",
    "                      n_fills=(\"H_rate_mean_w\", \"size\"))\n",
    "                 .reset_index())\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # E. plotting\n",
    "    # --------------------------------------------\n",
    "    for ipg in df_mean[\"ip_group\"].unique():\n",
    "        plot_ip_group(df_mean, plane=plane, ipg=ipg, err=\"sem\")\n",
    "\n",
    "    return df_mean, ip_fits\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_mean, fits = analyse_and_plot(\n",
    "        fills, beam, ip, RAWDATA, fbmodes, build_families_for_fill\n",
    "    )\n",
    "    df_mean.to_csv(\"ip_family_slopes.csv\")\n",
    "    #\n",
    "    # Example:\n",
    "    #\n",
    "    # from your_project import fbmodes, build_families_for_fill\n",
    "    # RAWDATA = Path(\"/path/to/rawdata/root\")\n",
    "    # fills   = [8450, 8451, 8452, ...]\n",
    "    # beam    = \"B1\"\n",
    "    # ip      = 1   # or however your build_families_for_fill uses it\n",
    "    #\n",
    "df_mean, fits = analyse_and_plot(\n",
    "         fills=fills,\n",
    "         beam=beam,\n",
    "         ip=ip,\n",
    "         rawdata=RAWDATA,\n",
    "         fbmodes=fbmodes,\n",
    "         build_families_for_fill=build_families_for_fill,\n",
    "     )\n",
    "    #\n",
    "df_mean.to_csv(\"family_ip_growth_rates.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4795d3",
   "metadata": {},
   "source": [
    "# STable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "stable_window_ip_groups.py\n",
    "--------------------------\n",
    "\n",
    "Cumulative INJPHYS/STABLE emittance‑growth analysis **per IP‑group**.\n",
    "\n",
    "External objects you need to provide in your notebook/script before\n",
    "calling `main()` or running the __main__ block:\n",
    "\n",
    "    fbmodes : DataFrame with fill‑mode metadata (columns tsStart/tsEnd/BMODE)\n",
    "    fills   : list[int]   – fills to process\n",
    "    beam    : \"B1\" | \"B2\"\n",
    "    build_families_for_fill(fill, beam, ip, RAWDATA) -> {family:[slots]}\n",
    "    build_family_ip_partitions(fams_by_fill, RAWDATA, beam)\n",
    "                              -> df_ip_long , df_ip_summary\n",
    "\n",
    "Required python packages: numpy, pandas, matplotlib, lmfit\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# CONFIG  – adjust paths/PVs once\n",
    "# ---------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lmfit import Model\n",
    "\n",
    "ip              = 1\n",
    "RAWDATA         = Path(\"/eos/project/l/lhc-lumimod/LuminosityFollowUp/2025/rawdata\")\n",
    "\n",
    "BETA_PV         = \"HX:BETASTAR_IP1\"\n",
    "XING_PV         = \"LhcStateTracker:LHCBEAM:IP1-XING-H-MURAD:value\"\n",
    "LEV_PV          = \"LHC.LUMISERVER:LumiLevelingIP1:Enable\"\n",
    "THRESH_CA       = 120.0          # μrad\n",
    "\n",
    "F_SCALE         = 0.1\n",
    "REL_ERR         = 0.05\n",
    "ABS_ERR_FLOOR   = 0.02\n",
    "MIN_ERR         = 1e-12\n",
    "MIN_PTS         = 5\n",
    "MIN_DT_H        = 0.15\n",
    "MAX_WEIGHT      = 1e4\n",
    "\n",
    "MAKE_HOURLY     = True           # include 1h,2h,… cumulative windows\n",
    "\n",
    "FIGSIZE_ERR     = (11, 4)\n",
    "FAMILY_COLORS   = {\n",
    "    \"Family_1\": \"#1f77b4\", \"Family_2\": \"#ff7f0e\", \"Family_3\": \"#2ca02c\",\n",
    "    \"Family_4\": \"#d62728\", \"Family_5\": \"#9467bd\", \"Family_6\": \"#8c564b\",\n",
    "    \"Family_7\": \"#e377c2\", \"Family_8\": \"#7f7f7f\",\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 0.  SMALL HELPERS  (generic I/O + fitting)\n",
    "# ---------------------------------------------------------------------\n",
    "def bsrt_loc_for_beam(b: str) -> str:\n",
    "    return \"5R4\" if b.upper() == \"B1\" else \"5L4\"\n",
    "\n",
    "def _to_tz(ts, tz):\n",
    "    return ts if ts is None else (ts if ts.tzinfo else ts.tz_localize(tz))\n",
    "\n",
    "def load_series(pv: str, fno: int, root: Path) -> pd.Series:\n",
    "    \"\"\"Load *any* scalar parquet PV for an entire fill (no mode cut).\"\"\"\n",
    "    parts = []\n",
    "    for pq in (root / f\"HX:FILLN={fno}\").rglob(\"*.parquet\"):\n",
    "        try:\n",
    "            df = pd.read_parquet(pq, columns=[pv])\n",
    "        except Exception:\n",
    "            continue\n",
    "        idx = pd.to_datetime(df.index, utc=True, errors=\"coerce\")\n",
    "        parts.append(pd.Series(df[pv].values, index=idx).dropna())\n",
    "    return pd.concat(parts, sort=True) if parts else pd.Series(dtype=float)\n",
    "\n",
    "def load_STABLE_series(pv: str, fno: int) -> pd.Series:\n",
    "    \"\"\"Return the STABLE‑beam segment for a PV.\"\"\"\n",
    "    sub = fbmodes.loc[fno].sort_values(\"tsStart\")\n",
    "    stable = sub[sub[\"BMODE\"].str.upper() == \"STABLE\"]\n",
    "    if stable.empty:\n",
    "        raise RuntimeError(f\"No STABLE period for fill {fno}\")\n",
    "    t0, t1 = map(pd.to_datetime, (stable[\"tsStart\"].iloc[0],\n",
    "                                  stable[\"tsEnd\"]  .iloc[-1]))\n",
    "    ser = load_series(pv, fno, RAWDATA)\n",
    "    ser = ser[~ser.index.duplicated(keep=\"first\")]\n",
    "    return ser.loc[t0:t1]\n",
    "\n",
    "def extract_slot(ser: pd.Series, slot: int) -> pd.Series:\n",
    "    return ser.map(lambda a: a[slot] if hasattr(a, \"__len__\") and len(a) > slot else np.nan)\n",
    "\n",
    "# robust linear fit ----------------------------------------------------\n",
    "_line_model = Model(lambda x, m, b: m * x + b)\n",
    "\n",
    "def robust_line(xh: np.ndarray, y: np.ndarray):\n",
    "    yerr = np.maximum(np.maximum(REL_ERR * np.abs(y), ABS_ERR_FLOOR), MIN_ERR)\n",
    "    w    = np.minimum(1.0 / yerr, MAX_WEIGHT)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "        r = _line_model.fit(\n",
    "            y, x=xh, m=0.0, b=y[0], weights=w,\n",
    "            method=\"least_squares\", fit_kws={\"loss\": \"soft_l1\", \"f_scale\": F_SCALE}\n",
    "        )\n",
    "    m, b = r.params[\"m\"].value, r.params[\"b\"].value\n",
    "    sm   = r.params[\"m\"].stderr or np.nan\n",
    "    return m, b, sm\n",
    "\n",
    "# weighted mean + sem --------------------------------------------------\n",
    "def _wmean_sem(pairs):\n",
    "    if not pairs: return np.nan, np.nan, 0\n",
    "    vals, errs = map(np.asarray, zip(*pairs))\n",
    "    ok = np.isfinite(vals) & np.isfinite(errs) & (errs > 0)\n",
    "    if ok.sum() == 0: return np.nan, np.nan, 0\n",
    "    w = 1.0 / errs[ok] ** 2\n",
    "    mean = np.sum(w * vals[ok]) / np.sum(w)\n",
    "    sem  = (1.0 / np.sum(w)) ** 0.5\n",
    "    return mean, sem, int(ok.sum())\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1.  MARKERS & WINDOWS  (β*, crab‑cavity etc.)\n",
    "# ---------------------------------------------------------------------\n",
    "def phase_markers(fno: int):\n",
    "    beta   = load_STABLE_series(BETA_PV, fno)\n",
    "    xing   = load_STABLE_series(XING_PV, fno)\n",
    "    ca0    = xing.iloc[0]\n",
    "\n",
    "    # β* final = start of longest constant β* plateau\n",
    "    runs, run_start, v_prev = [], beta.index[0], beta.iloc[0]\n",
    "    for t, v in beta.iloc[1:].items():\n",
    "        if v != v_prev:\n",
    "            runs.append((run_start, t))\n",
    "            run_start, v_prev = t, v\n",
    "    runs.append((run_start, beta.index[-1]))\n",
    "    t_beta_final = max(runs, key=lambda r: r[1] - r[0])[0]\n",
    "\n",
    "    # crossing‑angle events\n",
    "    idx_leave = np.where(xing.values != ca0)[0]\n",
    "    t_ca_start = xing.index[idx_leave[0]] if idx_leave.size else xing.index[-1]\n",
    "    idx_dip = np.where((xing.values <= THRESH_CA) &\n",
    "                       (xing.index > t_ca_start))[0]\n",
    "    t_ca_dip = xing.index[idx_dip[0]] if idx_dip.size else xing.index[-1]\n",
    "\n",
    "    t_lvl_on = leveling_on_time(fno)\n",
    "\n",
    "    return dict(beta_final=t_beta_final,\n",
    "                ca_start=t_ca_start,\n",
    "                ca_dip=t_ca_dip,\n",
    "                lvl_on=t_lvl_on)\n",
    "\n",
    "def leveling_on_time(fno: int) -> pd.Timestamp:\n",
    "    lev = load_series(LEV_PV, fno, RAWDATA).astype(float).dropna()\n",
    "    d   = lev.diff().fillna(0)\n",
    "    t_edge = d[d > 0].index.min()\n",
    "    t_val  = lev[lev > 0].index.min()\n",
    "    return min([t for t in (t_edge, t_val) if pd.notna(t)])\n",
    "\n",
    "def build_windows(t0, markers, ser_H, make_hourly=True):\n",
    "    \"\"\"Return list[(label, t_end)].  t0 is leveling‑on start.\"\"\"\n",
    "    eps = pd.Timedelta(seconds=1)\n",
    "    tz  = ser_H.index.tz\n",
    "    tend_data = ser_H.index.max()\n",
    "\n",
    "    wins = []\n",
    "    if make_hourly:\n",
    "        last = markers[\"beta_final\"] - pd.Timedelta(hours=1)\n",
    "        hour, current = 1, t0\n",
    "        while current + pd.Timedelta(hours=1) <= last:\n",
    "            current += pd.Timedelta(hours=1)\n",
    "            wins.append((f\"{hour}h since lvl-on\", current))\n",
    "            hour += 1\n",
    "\n",
    "    for lab in (\"beta_final\", \"ca_start\", \"ca_dip\"):\n",
    "        tend = markers[lab]\n",
    "        if tend - t0 > eps:\n",
    "            wins.append((f\"up to {('β*' if lab=='beta_final' else lab.replace('_',' '))}\", tend))\n",
    "\n",
    "    # clip & deduplicate\n",
    "    clean = []\n",
    "    for lab, t1 in wins:\n",
    "        t1c = min(t1, tend_data)\n",
    "        if t1c - t0 > eps:\n",
    "            clean.append((lab, t1c))\n",
    "    return clean\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2.  PER‑FILL, PER‑IP‑GROUP cumulative slopes\n",
    "# ---------------------------------------------------------------------\n",
    "def cumulative_rates_one_fill(\n",
    "    fno: int,\n",
    "    beam: str,\n",
    "    fams_ip: dict[str, list[int]],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Return DataFrame indexed by (window, family, plane, stat{mean,sem}).\"\"\"\n",
    "    markers = phase_markers(fno)\n",
    "    t0      = markers[\"lvl_on\"]\n",
    "\n",
    "    bsrt_loc = bsrt_loc_for_beam(beam)\n",
    "    ser_H = load_STABLE_series(f\"LHC.BSRT.{bsrt_loc}.{beam}:BUNCH_EMITTANCE_H\", fno)\n",
    "    ser_V = load_STABLE_series(f\"LHC.BSRT.{bsrt_loc}.{beam}:BUNCH_EMITTANCE_V\", fno)\n",
    "\n",
    "    wins = build_windows(t0, markers, ser_H, MAKE_HOURLY)\n",
    "\n",
    "    # raw slot data cache\n",
    "    rawH = {s: extract_slot(ser_H, s).sort_index() for slots in fams_ip.values() for s in slots}\n",
    "    rawV = {s: extract_slot(ser_V, s).sort_index() for slots in fams_ip.values() for s in slots}\n",
    "\n",
    "    recs = []\n",
    "\n",
    "    for label, t_end in wins:\n",
    "        for fam, slots in fams_ip.items():\n",
    "            H_pairs, V_pairs = [], []\n",
    "            for s in slots:\n",
    "                segH = rawH[s].loc[t0:t_end].dropna()\n",
    "                segV = rawV[s].loc[t0:t_end].dropna()\n",
    "                if len(segH) >= MIN_PTS:\n",
    "                    xh = (segH.index - t0).total_seconds() / 3600.0\n",
    "                    sh, _, sh_err = robust_line(xh, segH.values)\n",
    "                    if np.isfinite(sh_err) and sh_err > 0:\n",
    "                        H_pairs.append((sh, sh_err))\n",
    "                if len(segV) >= MIN_PTS:\n",
    "                    xv = (segV.index - t0).total_seconds() / 3600.0\n",
    "                    sv, _, sv_err = robust_line(xv, segV.values)\n",
    "                    if np.isfinite(sv_err) and sv_err > 0:\n",
    "                        V_pairs.append((sv, sv_err))\n",
    "\n",
    "            H_m, H_s, _ = _wmean_sem(H_pairs)\n",
    "            V_m, V_s, _ = _wmean_sem(V_pairs)\n",
    "\n",
    "            recs.append(dict(window=label, family=fam,\n",
    "                             H_mean=H_m, H_sem=H_s,\n",
    "                             V_mean=V_m, V_sem=V_s))\n",
    "    df = (pd.DataFrame(recs)\n",
    "            .set_index([\"window\", \"family\"])\n",
    "            .sort_index())\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3.  AGGREGATION OVER ALL FILLS  (averaging)\n",
    "# ---------------------------------------------------------------------\n",
    "def aggregate_over_fills(\n",
    "    df_tables: list[pd.DataFrame],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Input: list of DataFrames (index window×family, columns H_mean/H_sem/V_mean/V_sem)\n",
    "    Output: MultiIndex columns (family, plane, stat{mean,sem})\n",
    "    \"\"\"\n",
    "    big = pd.concat(df_tables, keys=range(len(df_tables)), names=[\"fill\"])\n",
    "    # build new column structure\n",
    "    cols = {}\n",
    "    for fam in big.index.get_level_values(\"family\").unique():\n",
    "        for plane in (\"H\", \"V\"):\n",
    "            mcol = (fam, plane, \"mean\")\n",
    "            scol = (fam, plane, \"sem\")\n",
    "            sub  = big.xs(fam, level=\"family\")[f\"{plane}_mean\"]\n",
    "            cols[mcol] = sub.mean(level=\"window\")\n",
    "            cols[scol] = sub.sem (level=\"window\")\n",
    "    df_out = pd.concat(cols, axis=1).sort_index(axis=1)\n",
    "    return df_out\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4.  PLOT (same style the user had)\n",
    "# ---------------------------------------------------------------------\n",
    "def split_windows_names(stable_avg: pd.DataFrame):\n",
    "    hourly = [w for w in stable_avg.index if \"h since\" in w]\n",
    "    optics = [w for w in (\"up to β*\", \"up to CA start\", \"up to CA dip\")\n",
    "              if w in stable_avg.index]\n",
    "    return hourly, optics\n",
    "\n",
    "def common_ylim(stable_avg, plane, pad=0.12):\n",
    "    cols = [c for c in stable_avg.columns if c[1] == plane and c[2] == \"mean\"]\n",
    "    vals = stable_avg[cols].to_numpy().ravel()\n",
    "    vals = vals[np.isfinite(vals)]\n",
    "    if vals.size == 0:\n",
    "        return None\n",
    "    lo, hi = vals.min(), vals.max()\n",
    "    span = hi - lo if hi > lo else 1.0\n",
    "    return lo - pad * span, hi + pad * span\n",
    "\n",
    "def draw_mean_band(ax, data, frac=0.25, color=\"k\", alpha=0.08):\n",
    "    m = np.nanmean(data)\n",
    "    band = frac * abs(m)\n",
    "    ax.axhline(m, color=color, lw=1.2, alpha=0.35)\n",
    "    ax.axhspan(m - band, m + band, color=color, alpha=alpha, zorder=0)\n",
    "\n",
    "def plot_stable_avg_split_lines(stable_avg: pd.DataFrame,\n",
    "                                plane=\"H\", pct_band=0.25,\n",
    "                                group_spacing=1.2, figsize=(13, 5)):\n",
    "    hourly, optics = split_windows_names(stable_avg)\n",
    "    fams = sorted({f for f, pl, st in stable_avg.columns if pl == plane})\n",
    "    base = dict(hourly=hourly, optics=optics)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize, sharey=True)\n",
    "    ylim = common_ylim(stable_avg, plane)\n",
    "    for ax, key, title in zip(axes, (\"hourly\", \"optics\"),\n",
    "                              (\"Hourly windows\", \"Optics phases\")):\n",
    "        wins = base[key]\n",
    "        x0 = np.arange(len(wins)) * group_spacing\n",
    "        all_means = []\n",
    "\n",
    "        for fam in fams:\n",
    "            m = stable_avg.loc[wins, (fam, plane, \"mean\")].to_numpy()\n",
    "            e = stable_avg.loc[wins, (fam, plane, \"sem\" )].to_numpy()\n",
    "            ax.errorbar(x0, m, yerr=e, fmt=\"-o\", capsize=3, ms=5,\n",
    "                        color=FAMILY_COLORS.get(fam), label=fam if ax is axes[0] else None)\n",
    "            all_means.append(m)\n",
    "\n",
    "        if all_means:\n",
    "            draw_mean_band(ax, np.concatenate(all_means), pct_band)\n",
    "\n",
    "        ax.set_xticks(x0); ax.set_xticklabels(wins, rotation=45, ha=\"right\")\n",
    "        ax.set_title(title); ax.grid(axis=\"y\", alpha=0.3)\n",
    "        if ylim: ax.set_ylim(*ylim)\n",
    "\n",
    "    axes[0].set_ylabel(f\"Growth rate ({plane})\")\n",
    "    axes[0].legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=\"small\")\n",
    "    fig.suptitle(f\"STABLE averages – {plane}-plane – Families across windows\")\n",
    "    fig.tight_layout(); plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5.  MAIN DRIVER\n",
    "# ---------------------------------------------------------------------\n",
    "def main():\n",
    "    # Step A: build families for every fill\n",
    "    fams_by_fill = {\n",
    "        f: build_families_for_fill(f, beam=beam, ip=ip, RAWDATA=RAWDATA)\n",
    "        for f in fills\n",
    "    }\n",
    "\n",
    "    # Step B: explode into slot‑wise IP table\n",
    "    df_ip_long, _ = build_family_ip_partitions(fams_by_fill, RAWDATA, beam)\n",
    "\n",
    "    # Step C: per‑(fill × IP) cumulative tables\n",
    "    per_ip_tables = {}               # ipg → list[DataFrame]\n",
    "    for fno in fills:\n",
    "        df_fill = df_ip_long.query(\"fill == @fno\")\n",
    "        for ipg, grp in df_fill.groupby(\"ip_group\"):\n",
    "            fams_ip = {fam: g[\"slot\"].to_numpy()\n",
    "                       for fam, g in grp.groupby(\"family\")}\n",
    "            df_one = cumulative_rates_one_fill(fno, beam, fams_ip)\n",
    "            per_ip_tables.setdefault(ipg, []).append(df_one)\n",
    "\n",
    "    # Step D: average over fills + plot\n",
    "    for ipg, tbls in per_ip_tables.items():\n",
    "        stable_avg = aggregate_over_fills(tbls)\n",
    "        print(f\"\\n=== IP‑group {ipg}  ({len(tbls)} fills) ===\")\n",
    "        display(stable_avg.head())\n",
    "        plot_stable_avg_split_lines(stable_avg, plane=\"H\")   # change to 'V' if needed\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure the external objects exist:\n",
    "    try:\n",
    "        fbmodes\n",
    "        fills\n",
    "        beam\n",
    "        build_families_for_fill\n",
    "        build_family_ip_partitions\n",
    "    except NameError as exc:\n",
    "        raise RuntimeError(\n",
    "            \"Before running this script you must define \"\n",
    "            \"`fbmodes`, `fills`, `beam`, \"\n",
    "            \"`build_families_for_fill`, and `build_family_ip_partitions`.\"\n",
    "        ) from exc\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
