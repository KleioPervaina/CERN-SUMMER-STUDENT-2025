{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68aa8c2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/eos/project/l/lhc-lumimod/LuminosityFollowUp/2025/rawdata/HX:FILLN=10688/HX:BMODE=RAMP",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 380\u001b[0m\n\u001b[1;32m    377\u001b[0m phase_times  \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fno \u001b[38;5;129;01min\u001b[39;00m fills:\n\u001b[0;32m--> 380\u001b[0m     fams \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_families_for_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRAWDATA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     tables_reg\u001b[38;5;241m.\u001b[39mappend(one_fill_family_rates_reg(fno, beam, fams))\n\u001b[1;32m    382\u001b[0m     stab_df, t_on, t_beta, t0_eff \u001b[38;5;241m=\u001b[39m compute_growth_raw_regression(fno, fams)\n",
      "Cell \u001b[0;32mIn[18], line 173\u001b[0m, in \u001b[0;36mbuild_families_for_fill\u001b[0;34m(fillno, beam, ip, RAWDATA, long_gap_set, small_gap, F4_offset_after_long, F6_offset_before_small, enforce_filled, intensity_threshold)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_families_for_fill\u001b[39m(fillno: \u001b[38;5;28mint\u001b[39m, beam: \u001b[38;5;28mstr\u001b[39m, ip: \u001b[38;5;28mstr\u001b[39m, RAWDATA: Path,\n\u001b[1;32m    170\u001b[0m                              long_gap_set\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m63\u001b[39m), small_gap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m    171\u001b[0m                              F4_offset_after_long\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, F6_offset_before_small\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m26\u001b[39m,\n\u001b[1;32m    172\u001b[0m                              enforce_filled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, intensity_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 173\u001b[0m     fpat \u001b[38;5;241m=\u001b[39m \u001b[43mLHCFillingPattern\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfillno\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRAWDATA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m beam\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB1\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    175\u001b[0m         trains \u001b[38;5;241m=\u001b[39m fpat\u001b[38;5;241m.\u001b[39mbunchtrainsDF_b1\u001b[38;5;241m.\u001b[39mcopy(); filled_slots \u001b[38;5;241m=\u001b[39m fpat\u001b[38;5;241m.\u001b[39mbunches_b1\n",
      "File \u001b[0;32m~/work/FILL10709/LHC_FillingPattern.py:36\u001b[0m, in \u001b[0;36mLHCFillingPattern.__init__\u001b[0;34m(self, fno, datadir, bunch_spacing)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatadir                \u001b[38;5;241m=\u001b[39m datadir\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbunch_spacing          \u001b[38;5;241m=\u001b[39m bunch_spacing \u001b[38;5;66;03m# bunch spacing in units of 25ns slots\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetBunchPatternAtBmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetInjections()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetHeadOn()\n",
      "File \u001b[0;32m~/work/FILL10709/LHC_FillingPattern.py:44\u001b[0m, in \u001b[0;36mLHCFillingPattern.setBunchPatternAtBmode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetBunchPatternAtBmode\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 44\u001b[0m     _tmp \u001b[38;5;241m=\u001b[39m \u001b[43mFilledBunchesAtBmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatadir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mLHCFillingPattern\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mLHCFillingPattern\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintensity_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfbunchesDF     \u001b[38;5;241m=\u001b[39m _tmp\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnobunches      \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB1\u001b[39m\u001b[38;5;124m'\u001b[39m: _tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB1\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnobunches\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB2\u001b[39m\u001b[38;5;124m'\u001b[39m : _tmp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB2\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnobunches\u001b[39m\u001b[38;5;124m'\u001b[39m] }\n",
      "File \u001b[0;32m~/work/FILL10709/LHC_FillingPattern.py:159\u001b[0m, in \u001b[0;36mFilledBunchesAtBmode\u001b[0;34m(fno, ddir, bmode, thres)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB2\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     var \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLHC.BCTFR.A6R4.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:BUNCH_INTENSITY\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 159\u001b[0m     fldbdf \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mddir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/HX:FILLN=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfno\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/HX:BMODE=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    160\u001b[0m     bunch_intensity \u001b[38;5;241m=\u001b[39m fldbdf\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m     filled_pattern \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3564\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.9/site-packages/dask_expr/_collection.py:5461\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, dtype_backend, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, blocksize, aggregate_files, parquet_file_extension, filesystem, engine, arrow_to_pandas, **kwargs)\u001b[0m\n\u001b[1;32m   5438\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   5439\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine is not supported when using the pyarrow filesystem.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5440\u001b[0m         )\n\u001b[1;32m   5442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\n\u001b[1;32m   5443\u001b[0m         ReadParquetPyarrowFS(\n\u001b[1;32m   5444\u001b[0m             path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5457\u001b[0m         )\n\u001b[1;32m   5458\u001b[0m     )\n\u001b[1;32m   5460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\n\u001b[0;32m-> 5461\u001b[0m     \u001b[43mReadParquetFSSpec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_convert_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalculate_divisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalculate_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5471\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5473\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparquet_file_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_set_parquet_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5478\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5479\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5480\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.9/site-packages/dask_expr/_core.py:57\u001b[0m, in \u001b[0;36mExpr.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m inst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m     56\u001b[0m inst\u001b[38;5;241m.\u001b[39moperands \u001b[38;5;241m=\u001b[39m [_unpack_collections(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m operands]\n\u001b[0;32m---> 57\u001b[0m _name \u001b[38;5;241m=\u001b[39m \u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _name \u001b[38;5;129;01min\u001b[39;00m Expr\u001b[38;5;241m.\u001b[39m_instances:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Expr\u001b[38;5;241m.\u001b[39m_instances[_name]\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.9/functools.py:993\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    991\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 993\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    995\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.9/site-packages/dask_expr/io/parquet.py:778\u001b[0m, in \u001b[0;36mReadParquet._name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_name\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_funcname\n\u001b[1;32m    776\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    777\u001b[0m         \u001b[38;5;241m+\u001b[39m _tokenize_deterministic(\n\u001b[0;32m--> 778\u001b[0m             funcname(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchecksum\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    779\u001b[0m         )\n\u001b[1;32m    780\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.9/site-packages/dask_expr/io/parquet.py:784\u001b[0m, in \u001b[0;36mReadParquet.checksum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchecksum\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_info\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchecksum\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.9/site-packages/dask_expr/io/parquet.py:1356\u001b[0m, in \u001b[0;36mReadParquetFSSpec._dataset_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;66;03m# Collect general dataset info\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1338\u001b[0m     paths,\n\u001b[1;32m   1339\u001b[0m     fs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1354\u001b[0m     },\n\u001b[1;32m   1355\u001b[0m )\n\u001b[0;32m-> 1356\u001b[0m dataset_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_dataset_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1357\u001b[0m checksum \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1358\u001b[0m files_for_checksum \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py:1061\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# Final \"catch-all\" pyarrow.dataset call\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1061\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mpa_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_wrapped_fs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_processed_dataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;66;03m# Get file_frag sample and extract physical_schema\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.9/site-packages/pyarrow/dataset.py:797\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n\u001b[0;32m--> 797\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(elem, Dataset) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n\u001b[1;32m    799\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _union_dataset(source, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.9/site-packages/pyarrow/dataset.py:474\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    472\u001b[0m         paths_or_selector \u001b[38;5;241m=\u001b[39m source\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 474\u001b[0m         fs, paths_or_selector \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_multiple_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m _ensure_single_source(source, filesystem)\n",
      "File \u001b[0;32m~/miniforge3/envs/myenv/lib/python3.9/site-packages/pyarrow/dataset.py:382\u001b[0m, in \u001b[0;36m_ensure_multiple_sources\u001b[0;34m(paths, filesystem)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mNotFound:\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(info\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mDirectory:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m(\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m points to a directory, but only file paths are \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupported. To construct a nested or union dataset pass \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma list of dataset objects instead.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(info\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m    388\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /eos/project/l/lhc-lumimod/LuminosityFollowUp/2025/rawdata/HX:FILLN=10688/HX:BMODE=RAMP"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Emittance growth pipeline (robust, despiked, fit-based)\n",
    "-------------------------------------------------------\n",
    "- Cleans slot time series (median/MAD despike + optional Gaussian smoothing)\n",
    "- Robust linear fits (Theil–Sen) for injection & stable windows\n",
    "- Uses ONLY the fit values to compute growth between end of injection and start of stable\n",
    "- Plots per-family growth for all requested fills\n",
    "\n",
    "Recursion/Assertion safe: we fully replace FillInjectionsForBeam, never calling the original.\n",
    "\"\"\"\n",
    "\n",
    "# ===================== USER CONFIG =====================================\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress, theilslopes\n",
    "\n",
    "fills = [10671, 10672, 10673, 10674, 10676, 10677, 10678, 10683,\n",
    "         10688, 10689, 10690, 10691, 10696, 10697, 10700, 10701,\n",
    "         10702, 10703, 10704, 10705, 10709, 10713, 10716, 10718,\n",
    "         10721, 10731, 10732]\n",
    "\n",
    "beam    = \"B1\"\n",
    "ip      = \"1\"   # string\n",
    "RAWDATA = Path('/eos/project/l/lhc-lumimod/LuminosityFollowUp/2025/rawdata')\n",
    "FILLINFO= Path('/eos/project/l/lhc-lumimod/LuminosityFollowUp/2025/fills-info')\n",
    "fbmodes = pd.read_parquet(FILLINFO / 'fills_and_bmodes_2025.parquet')\n",
    "\n",
    "# PVs\n",
    "int_var    = f\"LHC.BCTFR.B6R4.{beam}:BUNCH_INTENSITY\"\n",
    "emitH_var  = f\"LHC.BSRT.5R4.{beam}:BUNCH_EMITTANCE_H\"\n",
    "emitV_var  = f\"LHC.BSRT.5R4.{beam}:BUNCH_EMITTANCE_V\"\n",
    "lev_pv     = \"LHC.LUMISERVER:LumiLevelingIP1:Enable\"\n",
    "beta_pv    = \"HX:BETASTAR_IP1\"\n",
    "xing_pv    = \"LhcStateTracker:LHCBEAM:IP1-XING-H-MURAD:value\"\n",
    "\n",
    "# Parameters\n",
    "INTENSITY_THRESHOLD = 1e10\n",
    "TOL_MINUTES         = 5\n",
    "DESPIKE_K           = 5.0\n",
    "DESPIKE_WIN         = '5min'\n",
    "GAUSS_SIGMA_PTS     = 0\n",
    "ROBUST_FIT_FUNC     = 'theilsen'   # or 'ols'\n",
    "# =======================================================================\n",
    "\n",
    "# ---------------- MONKEY-PATCH (direct replacement) --------------------\n",
    "import LHC_FillingPattern as _lfp\n",
    "from pathlib import Path as _P\n",
    "\n",
    "def _safe_concat_sort(dflist):\n",
    "    if not dflist:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat(dflist, sort=True)\n",
    "    if not df.empty:\n",
    "        df = df.sort_index()\n",
    "        df = df[~df.index.duplicated(keep='first')]\n",
    "    return df\n",
    "\n",
    "def FillInjectionsForBeam_direct(fno, ddir, beam, threshold):\n",
    "    \"\"\"Direct parquet reader for bunch intensity, sorted & deduped.\"\"\"\n",
    "    root = _P(ddir) / f\"HX:FILLN={fno}\"\n",
    "    col  = f\"LHC.BCTFR.B6R4.{beam}:BUNCH_INTENSITY\"\n",
    "    dflist = []\n",
    "    for pq in root.rglob('*.parquet'):\n",
    "        try:\n",
    "            tmp = pd.read_parquet(pq, columns=[col])\n",
    "        except Exception:\n",
    "            continue\n",
    "        tmp.index = pd.to_datetime(tmp.index, utc=True, errors='coerce')\n",
    "        dflist.append(tmp.dropna())\n",
    "    return _safe_concat_sort(dflist)\n",
    "\n",
    "# Patch\n",
    "_lfp.FillInjectionsForBeam = FillInjectionsForBeam_direct\n",
    "from LHC_FillingPattern import LHCFillingPattern\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# --------------------- UTILITIES ---------------------------------------\n",
    "\n",
    "def load_series(pv: str, fno: int, root: pathlib.Path) -> pd.Series:\n",
    "    parts = []\n",
    "    fill_dir = root / f\"HX:FILLN={fno}\"\n",
    "    for pq in fill_dir.rglob('*.parquet'):\n",
    "        try:\n",
    "            df = pd.read_parquet(pq, columns=[pv])\n",
    "        except Exception:\n",
    "            continue\n",
    "        idx = pd.to_datetime(df.index, utc=True, errors='coerce')\n",
    "        parts.append(pd.Series(df[pv].values, index=idx, name=pv).dropna())\n",
    "    return pd.concat(parts, sort=True) if parts else pd.Series(dtype=float, name=pv)\n",
    "\n",
    "\n",
    "def load_INJPHYS(fno: int, var: str) -> pd.Series:\n",
    "    if fno not in fbmodes.index:\n",
    "        return pd.Series(dtype=float)\n",
    "    sub = fbmodes.loc[fno]\n",
    "    if isinstance(sub, pd.DataFrame):\n",
    "        rows = sub.query(\"BMODE=='INJPHYS'\").sort_values('tsStart')\n",
    "    else:\n",
    "        rows = pd.DataFrame([sub]).query(\"BMODE=='INJPHYS'\")\n",
    "    if rows.empty:\n",
    "        rows = sub.sort_values('tsStart').iloc[[0]] if isinstance(sub, pd.DataFrame) else pd.DataFrame([sub])\n",
    "    t0 = pd.to_datetime(rows['tsStart'].iloc[0], utc=True)\n",
    "    t1 = pd.to_datetime(rows['tsEnd'  ].iloc[0], utc=True)\n",
    "    s = load_series(var, fno, RAWDATA)\n",
    "    return s.sort_index().loc[t0:t1].dropna()\n",
    "\n",
    "\n",
    "def phase_markers_from_beta_xing(ser_beta: pd.Series, ser_xing: pd.Series, xing_threshold: float = 120.0):\n",
    "    if ser_beta.empty or ser_xing.empty:\n",
    "        return pd.NaT, pd.NaT, pd.NaT\n",
    "    runs = []\n",
    "    t_prev, v_prev = ser_beta.index[0], ser_beta.iloc[0]\n",
    "    run_start = t_prev\n",
    "    for t, v in ser_beta.iloc[1:].items():\n",
    "        if v != v_prev:\n",
    "            runs.append((run_start, t_prev))\n",
    "            run_start, v_prev = t, v\n",
    "        t_prev = t\n",
    "    runs.append((run_start, t_prev))\n",
    "    t_beta_final = max(runs, key=lambda x: x[1] - x[0])[0]\n",
    "\n",
    "    ca0   = ser_xing.iloc[0]\n",
    "    leave = ser_xing.index[(ser_xing != ca0).argmax()] if (ser_xing != ca0).any() else ser_xing.index[-1]\n",
    "    dip   = ser_xing.index[(ser_xing <= xing_threshold).argmax()] if (ser_xing <= xing_threshold).any() else ser_xing.index[-1]\n",
    "    return t_beta_final, leave, dip\n",
    "\n",
    "\n",
    "def extract_slot(ser: pd.Series, slot: int) -> pd.Series:\n",
    "    return ser.map(lambda a: a[slot] if hasattr(a,'__len__') and len(a)>slot else np.nan).dropna()\n",
    "\n",
    "# Cleaning\n",
    "\n",
    "def despike(series: pd.Series, k: float = DESPIKE_K, win: str = DESPIKE_WIN) -> pd.Series:\n",
    "    if series.empty:\n",
    "        return series\n",
    "    med = series.rolling(win, min_periods=1).median()\n",
    "    mad = (series - med).abs().rolling(win, min_periods=1).median()\n",
    "    z = (series - med) / mad.replace(0, np.nan)\n",
    "    cleaned = series.mask(z.abs() > k)\n",
    "    return cleaned.interpolate(limit_direction='both')\n",
    "\n",
    "try:\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    def gaussian_smooth(series: pd.Series, sigma_pts: int = GAUSS_SIGMA_PTS) -> pd.Series:\n",
    "        if sigma_pts <= 0 or series.empty:\n",
    "            return series\n",
    "        y = gaussian_filter1d(series.values, sigma=sigma_pts, mode='nearest')\n",
    "        return pd.Series(y, index=series.index)\n",
    "except Exception:\n",
    "    def gaussian_smooth(series: pd.Series, sigma_pts: int = GAUSS_SIGMA_PTS) -> pd.Series:\n",
    "        return series\n",
    "\n",
    "# Robust fit\n",
    "\n",
    "def robust_linfit(x: np.ndarray, y: np.ndarray):\n",
    "    if ROBUST_FIT_FUNC == 'theilsen':\n",
    "        slope, intercept, _, _ = theilslopes(y, x)\n",
    "    else:\n",
    "        r = linregress(x, y)\n",
    "        slope, intercept = r.slope, r.intercept\n",
    "    return slope, intercept\n",
    "\n",
    "# ------------------ Families builder -----------------------------------\n",
    "\n",
    "def build_families_for_fill(fillno: int, beam: str, ip: str, RAWDATA: Path,\n",
    "                             long_gap_set=(32,63), small_gap=8,\n",
    "                             F4_offset_after_long=6, F6_offset_before_small=26,\n",
    "                             enforce_filled=True, intensity_threshold=None) -> dict:\n",
    "    fpat = LHCFillingPattern(fillno, RAWDATA)\n",
    "    if beam.upper() == 'B1':\n",
    "        trains = fpat.bunchtrainsDF_b1.copy(); filled_slots = fpat.bunches_b1\n",
    "    else:\n",
    "        trains = fpat.bunchtrainsDF_b2.copy(); filled_slots = fpat.bunches_b2\n",
    "    trains = trains.loc[trains['id'] != 0].sort_values('bid_first').reset_index(drop=True)\n",
    "    orbit_len = 3564\n",
    "\n",
    "    if intensity_threshold is not None:\n",
    "        ser_int = load_series(int_var, fillno, RAWDATA)\n",
    "        if ser_int.empty:\n",
    "            arr_intensity = np.zeros(orbit_len)\n",
    "        else:\n",
    "            last_vals = np.asarray(ser_int.iloc[-1])\n",
    "            if last_vals.shape[0] != orbit_len:\n",
    "                tmp = np.zeros(orbit_len); n = min(orbit_len, last_vals.shape[0])\n",
    "                tmp[:n] = last_vals[:n]; last_vals = tmp\n",
    "            arr_intensity = last_vals\n",
    "    else:\n",
    "        arr_intensity = None\n",
    "\n",
    "    filled_mask = np.zeros(orbit_len, dtype=bool); filled_mask[filled_slots] = True\n",
    "    def _filter(arr: np.ndarray) -> np.ndarray:\n",
    "        if arr.size == 0:\n",
    "            return arr\n",
    "        arr = arr.astype(int)\n",
    "        if enforce_filled:\n",
    "            arr = arr[filled_mask[arr]]\n",
    "        if arr_intensity is not None and intensity_threshold is not None:\n",
    "            arr = arr[arr_intensity[arr] >= intensity_threshold]\n",
    "        return arr\n",
    "\n",
    "    mask_long = trains['gap'].isin(long_gap_set)\n",
    "    Family_1 = trains.loc[mask_long, 'bid_first'].to_numpy(int)\n",
    "\n",
    "    next_start = trains['bid_first'].shift(-1).fillna(orbit_len).astype(int)\n",
    "    trains['gap_after'] = next_start - trains['bid_last'] - 1\n",
    "    Family_2 = trains.loc[trains['gap_after'] >= 31, 'bid_last'].to_numpy(int)\n",
    "\n",
    "    lr = fpat.lrencounters[beam][f\"ip{ip}\"]\n",
    "    peak_slots = []\n",
    "    for _, row in trains.iterrows():\n",
    "        bids = np.asarray(row['bids'], dtype=int)\n",
    "        lr_train = lr[bids]\n",
    "        first_idx = np.where(lr_train == lr_train.max())[0][0]\n",
    "        peak_slots.append(bids[first_idx])\n",
    "    Family_3 = np.asarray(peak_slots, dtype=int)\n",
    "\n",
    "    Family_4 = (trains.loc[mask_long, 'bid_first'] + F4_offset_after_long).to_numpy(int)\n",
    "\n",
    "    mask_gap_small_15 = (trains['gap'] == small_gap) & (trains['nbunches'] >= 15)\n",
    "    cand_F5 = (trains.loc[mask_gap_small_15, 'bid_first'] + 14).to_numpy(int)\n",
    "    last_F5 = trains.loc[mask_gap_small_15, 'bid_last'].to_numpy(int)\n",
    "    Family_5 = cand_F5[cand_F5 <= last_F5]\n",
    "\n",
    "    rows_gap8 = trains['gap'] == small_gap\n",
    "    train_before_gap8 = trains.shift(1).loc[rows_gap8].dropna()\n",
    "    mask_nbunch27 = train_before_gap8['nbunches'] >= (F6_offset_before_small + 1)\n",
    "    Family_6 = (train_before_gap8.loc[mask_nbunch27, 'bid_first'] + F6_offset_before_small).to_numpy(int)\n",
    "\n",
    "    Family_7 = trains.loc[rows_gap8, 'bid_first'].to_numpy(int)\n",
    "\n",
    "    train_before_gap8_full = trains.shift(1).loc[rows_gap8].dropna()\n",
    "    Family_8 = train_before_gap8_full['bid_last'].astype(int).to_numpy()\n",
    "\n",
    "    return {\n",
    "        'Family_1': _filter(Family_1),\n",
    "        'Family_2': _filter(Family_2),\n",
    "        'Family_3': _filter(Family_3),\n",
    "        'Family_4': _filter(Family_4),\n",
    "        'Family_5': _filter(Family_5),\n",
    "        'Family_6': _filter(Family_6),\n",
    "        'Family_7': _filter(Family_7),\n",
    "        'Family_8': _filter(Family_8),\n",
    "    }\n",
    "\n",
    "# ---------------- Injection regression ---------------------------------\n",
    "\n",
    "def one_fill_family_rates_reg(fno: int, beam: str, families: dict, threshold: float = INTENSITY_THRESHOLD) -> pd.DataFrame:\n",
    "    I  = load_INJPHYS(fno, int_var)\n",
    "    EH = load_INJPHYS(fno, emitH_var)\n",
    "    EV = load_INJPHYS(fno, emitV_var)\n",
    "\n",
    "    per = {\"H_slope\":{}, \"H_int\":{}, \"V_slope\":{}, \"V_int\":{}, \"t0\":{}}\n",
    "    for fam, slots in families.items():\n",
    "        for slot in slots:\n",
    "            Ii = extract_slot(I, slot)\n",
    "            above = Ii[Ii >= threshold]\n",
    "            if above.empty:\n",
    "                for k in (\"H_slope\",\"H_int\",\"V_slope\",\"V_int\"):\n",
    "                    per[k][slot] = np.nan\n",
    "                per['t0'][slot] = pd.NaT\n",
    "                continue\n",
    "            t0 = above.index[0]\n",
    "            per['t0'][slot] = t0\n",
    "\n",
    "            Hs = extract_slot(EH, slot).loc[t0:].sort_index()\n",
    "            Vs = extract_slot(EV, slot).loc[t0:].sort_index()\n",
    "            Hs = gaussian_smooth(despike(Hs))\n",
    "            Vs = gaussian_smooth(despike(Vs))\n",
    "            if Hs.dropna().size < 2 or Vs.dropna().size < 2:\n",
    "                for k in (\"H_slope\",\"H_int\",\"V_slope\",\"V_int\"):\n",
    "                    per[k][slot] = np.nan\n",
    "                continue\n",
    "\n",
    "            xh = (Hs.index - t0).total_seconds()/3600.0\n",
    "            xv = (Vs.index - t0).total_seconds()/3600.0\n",
    "            mh, bh = robust_linfit(xh.values, Hs.values)\n",
    "            mv, bv = robust_linfit(xv.values, Vs.values)\n",
    "            per['H_slope'][slot] = mh; per['H_int'][slot] = bh\n",
    "            per['V_slope'][slot] = mv; per['V_int'][slot] = bv\n",
    "\n",
    "    rows = []\n",
    "    for fam, slots in families.items():\n",
    "        def avg(arr):\n",
    "            arr = [a for a in arr if not np.isnan(a)]\n",
    "            return np.mean(arr) if arr else np.nan\n",
    "        rows.append({\n",
    "            'fill': fno,\n",
    "            'family': fam,\n",
    "            'H_rate_avg_reg': avg([per['H_slope'][s] for s in slots]),\n",
    "            'H_int_avg_reg' : avg([per['H_int'][s]   for s in slots]),\n",
    "            'V_rate_avg_reg': avg([per['V_slope'][s] for s in slots]),\n",
    "            'V_int_avg_reg' : avg([per['V_int'][s]   for s in slots]),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---------------- Stable regression ------------------------------------\n",
    "\n",
    "def compute_growth_raw_regression(fno: int, families: dict):\n",
    "    ser_beta = load_series(beta_pv, fno, RAWDATA)\n",
    "    ser_xing = load_series(xing_pv, fno, RAWDATA)\n",
    "    t_beta_final, t_xing_begin, t_xing_end = phase_markers_from_beta_xing(ser_beta, ser_xing)\n",
    "\n",
    "    ser_lev = load_series(lev_pv, fno, RAWDATA).astype(float).dropna()\n",
    "    dlev    = ser_lev.diff().fillna(0)\n",
    "    t_on    = dlev[dlev>0].index.min() or ser_lev.index.min()\n",
    "\n",
    "    ser_H = load_series(emitH_var, fno, RAWDATA)\n",
    "    ser_V = load_series(emitV_var, fno, RAWDATA)\n",
    "    rawH  = {s: extract_slot(ser_H, s).sort_index() for fam in families for s in families[fam]}\n",
    "    rawV  = {s: extract_slot(ser_V, s).sort_index() for fam in families for s in families[fam]}\n",
    "    for s in rawH: rawH[s] = gaussian_smooth(despike(rawH[s]))\n",
    "    for s in rawV: rawV[s] = gaussian_smooth(despike(rawV[s]))\n",
    "\n",
    "    all_raw = pd.concat(list(rawH.values()) + list(rawV.values()), axis=1) if rawH else pd.DataFrame()\n",
    "    t0_eff  = all_raw.loc[t_on:].dropna(how='all').index.min() if not all_raw.empty else t_on\n",
    "\n",
    "    windows = []\n",
    "    prev, hr = t_on, 1\n",
    "    if pd.notna(t_beta_final):\n",
    "        for mark in pd.date_range(prev + pd.Timedelta(hours=1), t_beta_final, freq='1h'):\n",
    "            windows.append((f\"{hr}h since lvl-on\", prev, mark)); prev, hr = mark, hr+1\n",
    "        if prev < t_beta_final:\n",
    "            windows.append((\"to β* final\", prev, t_beta_final)); prev = t_beta_final\n",
    "        windows.append((\"β*→CA start\", prev, t_xing_begin))\n",
    "        windows.append((\"CA start→end\", t_xing_begin, t_xing_end))\n",
    "    else:\n",
    "        windows.append((\"lvl-on→end\", t_on, t_on + pd.Timedelta(hours=1)))\n",
    "\n",
    "    tol = pd.Timedelta(minutes=TOL_MINUTES)\n",
    "    rows = []\n",
    "    for label, _, t1 in windows:\n",
    "        t1b = pd.to_datetime(t1, utc=True).ceil('5min') if pd.notna(t1) else t_on\n",
    "        if pd.notna(t_xing_end):\n",
    "            t1b = min(t1b, t_xing_end.ceil('5min'))\n",
    "        dt  = (t1b - t0_eff).total_seconds()/3600.0\n",
    "        if dt <= 0:\n",
    "            continue\n",
    "        row = {'fill': fno, 'window': label}\n",
    "        for fam, slots in families.items():\n",
    "            sh, hints, sv, vints = [], [], [], []\n",
    "            for s in slots:\n",
    "                segH = rawH.get(s, pd.Series()).loc[t0_eff:t1b]\n",
    "                if len(segH)>=2 and (segH.index[0]-t0_eff)<=tol:\n",
    "                    xh = (segH.index - t0_eff).total_seconds()/3600\n",
    "                    mh, bh = robust_linfit(xh.values, segH.values)\n",
    "                    sh.append(mh); hints.append(bh)\n",
    "                segV = rawV.get(s, pd.Series()).loc[t0_eff:t1b]\n",
    "                    \n",
    "                if len(segV)>=2 and (segV.index[0]-t0_eff)<=tol:\n",
    "                    xv = (segV.index - t0_eff).total_seconds()/3600\n",
    "                    mv, bv = robust_linfit(xv.values, segV.values)\n",
    "                    sv.append(mv); vints.append(bv)\n",
    "            row[f\"{fam}_H_rate\"]      = np.mean(sh)    if sh    else np.nan\n",
    "            row[f\"{fam}_H_intercept\"] = np.mean(hints) if hints else np.nan\n",
    "            row[f\"{fam}_V_rate\"]      = np.mean(sv)    if sv    else np.nan\n",
    "            row[f\"{fam}_V_intercept\"] = np.mean(vints) if vints else np.nan\n",
    "        rows.append(row)\n",
    "\n",
    "    rows_df = pd.DataFrame(rows)\n",
    "    if rows_df.empty:\n",
    "        rows_df = pd.DataFrame(columns=['fill','window'])\n",
    "    if 'fill' not in rows_df.columns:\n",
    "        rows_df['fill'] = fno\n",
    "    if 'window' not in rows_df.columns:\n",
    "        rows_df['window'] = np.nan\n",
    "    rows_df = rows_df.set_index(['fill','window'])\n",
    "    return rows_df, t_on, t_beta_final, t0_eff\n",
    "\n",
    "# ---------------- MAIN BUILD -------------------------------------------\n",
    "\n",
    "tables_reg   = []\n",
    "all_results_1= {}\n",
    "phase_times  = {}\n",
    "\n",
    "for fno in fills:\n",
    "    fams = build_families_for_fill(fno, beam, ip, RAWDATA)\n",
    "    tables_reg.append(one_fill_family_rates_reg(fno, beam, fams))\n",
    "    stab_df, t_on, t_beta, t0_eff = compute_growth_raw_regression(fno, fams)\n",
    "    all_results_1[fno] = stab_df\n",
    "    phase_times[fno]   = (t_on, t_beta, t0_eff)\n",
    "\n",
    "df_inj_rates_reg = (pd.concat(tables_reg, ignore_index=True)\n",
    "                      .set_index(['fill','family'])\n",
    "                      .sort_index())\n",
    "\n",
    "# ---------------- GROWTH FROM FITS -------------------------------------\n",
    "\n",
    "growth_records = []\n",
    "for fno in fills:\n",
    "    t_on, t_beta, t0_eff = phase_times.get(fno, (pd.NaT, pd.NaT, pd.NaT))\n",
    "    if pd.isna(t_on) or pd.isna(t_beta):\n",
    "        continue\n",
    "    dt_gap_h = (t_beta - t_on).total_seconds()/3600.0\n",
    "    if fno not in df_inj_rates_reg.index.get_level_values('fill'):\n",
    "        continue\n",
    "    inj_df = df_inj_rates_reg.xs(fno, level='fill')\n",
    "    if inj_df.empty:\n",
    "        continue\n",
    "    try:\n",
    "        stab_df = all_results_1[fno].xs('β*→CA start', level='window')\n",
    "    except (KeyError, ValueError):\n",
    "        continue\n",
    "\n",
    "    for fam in inj_df.index:\n",
    "        if fam not in stab_df.index:\n",
    "            continue\n",
    "        mH_inj = inj_df.loc[fam, 'H_rate_avg_reg']; bH_inj = inj_df.loc[fam, 'H_int_avg_reg']\n",
    "        mV_inj = inj_df.loc[fam, 'V_rate_avg_reg']; bV_inj = inj_df.loc[fam, 'V_int_avg_reg']\n",
    "        eps_H_end = bH_inj\n",
    "        eps_V_end = bV_inj\n",
    "\n",
    "        mH_st = stab_df.loc[fam, f'{fam}_H_rate'];      bH_st = stab_df.loc[fam, f'{fam}_H_intercept']\n",
    "        mV_st = stab_df.loc[fam, f'{fam}_V_rate'];      bV_st = stab_df.loc[fam, f'{fam}_V_intercept']\n",
    "        dt_st_h = (t_beta - t0_eff).total_seconds()/3600.0\n",
    "        eps_H_start = bH_st + mH_st * dt_st_h\n",
    "        eps_V_start = bV_st + mV_st * dt_st_h\n",
    "\n",
    "        growth_records.append({\n",
    "            'fill':   fno,\n",
    "            'family': fam,\n",
    "            'H_growth': (eps_H_start - eps_H_end)/dt_gap_h,\n",
    "            'V_growth': (eps_V_start - eps_V_end)/dt_gap_h,\n",
    "        })\n",
    "\n",
    "df_growth = pd.DataFrame(growth_records).set_index(['fill','family']).sort_index()\n",
    "\n",
    "# ---------------- PLOTS -------------------------------------------------\n",
    "for plane in ('H','V'):\n",
    "    col = f'{plane}_growth'\n",
    "    if col not in df_growth.columns:\n",
    "        continue\n",
    "    piv = df_growth[col].unstack('fill')\n",
    "    ax = piv.plot(figsize=(9,4), marker='o')\n",
    "    ax.set_title(f'{plane}-plane Avg Emittance Growth (robust fits, despiked)')\n",
    "    ax.set_xlabel('Family')\n",
    "    ax.set_ylabel('Growth (units/hour)')\n",
    "    ax.legend(title='Fill', bbox_to_anchor=(1.02,1), loc='upper left')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# df_growth is the final result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
